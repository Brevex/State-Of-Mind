{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dependencies"
      ],
      "metadata": {
        "id": "Kw-zshHL-oYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets hmmlearn nltk"
      ],
      "metadata": {
        "id": "_pAM3nIvZbmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39788b34-d74a-4230-8ca7-efeb5f64a5bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/166.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "O3O-T70Dl8d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from hmmlearn import hmm\n",
        "from nltk.corpus import stopwords\n",
        "from datasets import load_dataset\n",
        "from dataclasses import dataclass\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "DjLyEPBvTyrw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# implementation with HMM\n"
      ],
      "metadata": {
        "id": "yXgCMhn5r8_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HP = {\n",
        "    'MAX_WORDS': 10000,\n",
        "    'DIM': 300,\n",
        "    'HMM_STATES': 3,\n",
        "    'LIMIT_TRAIN': 4000,\n",
        "    'LIMIT_TEST': 500\n",
        "}\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "def load_glove(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        return {line.split()[0]: np.asarray(line.split()[1:], dtype='float32')\n",
        "                for line in f}\n",
        "\n",
        "embeddings_index = load_glove(f'glove.6B.{HP[\"DIM\"]}d.txt')"
      ],
      "metadata": {
        "id": "YkIl2iPBT8D6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train_raw, y_train), (x_test_raw, y_test) = tf.keras.datasets.imdb.load_data(num_words=HP['MAX_WORDS'])\n",
        "\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "stop_words = set(stopwords.words('english')) | {'no', 'not', 'nor', 'neither', 'never', \"didn't\", \"isn't\"}\n",
        "embedding_matrix = np.zeros((HP['MAX_WORDS'] + 4, HP['DIM']))\n",
        "\n",
        "valid_indices = set()\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < HP['MAX_WORDS'] and word in embeddings_index and word not in stop_words:\n",
        "        embedding_matrix[i + 3] = embeddings_index[word]\n",
        "        valid_indices.add(i + 3)\n",
        "\n",
        "def vectorize_sequences(sequences, max_len=100):\n",
        "    processed_seqs = []\n",
        "    lengths = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        valid_tokens = [idx for idx in seq if idx in valid_indices][:max_len]\n",
        "\n",
        "        if valid_tokens:\n",
        "            processed_seqs.append(embedding_matrix[valid_tokens])\n",
        "            lengths.append(len(valid_tokens))\n",
        "\n",
        "    return processed_seqs, lengths\n",
        "\n",
        "X_train_seqs, train_lens = vectorize_sequences(x_train_raw[:HP['LIMIT_TRAIN']])\n",
        "X_test_seqs, test_lens = vectorize_sequences(x_test_raw[:HP['LIMIT_TEST']])\n",
        "\n",
        "y_train_sub = y_train[:HP['LIMIT_TRAIN']][:len(X_train_seqs)]\n",
        "y_test_sub = y_test[:HP['LIMIT_TEST']][:len(X_test_seqs)]"
      ],
      "metadata": {
        "id": "Ez4MZSlEUA38"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HMMSentiment:\n",
        "    def __init__(self, n_components=3):\n",
        "        common_params = {'n_components': n_components, 'covariance_type': \"diag\", 'n_iter': 20, 'verbose': False}\n",
        "\n",
        "        self.models = {\n",
        "            0: hmm.GaussianHMM(**common_params),\n",
        "            1: hmm.GaussianHMM(**common_params)\n",
        "        }\n",
        "\n",
        "    def fit(self, X_seqs, lengths, y):\n",
        "        for label, model in self.models.items():\n",
        "            indices = [i for i, t in enumerate(y) if t == label]\n",
        "\n",
        "            if not indices: continue\n",
        "\n",
        "            X_concat = np.concatenate([X_seqs[i] for i in indices])\n",
        "            lens_concat = [lengths[i] for i in indices]\n",
        "            model.fit(X_concat, lens_concat)\n",
        "\n",
        "    def predict_score_diff(self, X_seqs):\n",
        "        scores = []\n",
        "\n",
        "        for seq in X_seqs:\n",
        "            try:\n",
        "                s_pos = self.models[1].score(seq)\n",
        "                s_neg = self.models[0].score(seq)\n",
        "                scores.append(s_pos - s_neg)\n",
        "            except ValueError:\n",
        "                scores.append(0)\n",
        "\n",
        "        return np.array(scores)\n",
        "\n",
        "model = HMMSentiment(n_components=HP['HMM_STATES'])\n",
        "model.fit(X_train_seqs, train_lens, y_train_sub)"
      ],
      "metadata": {
        "id": "bH26hpTVUY23"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict_score_diff(X_test_seqs)\n",
        "\n",
        "thresholds = np.linspace(np.percentile(scores, 5), np.percentile(scores, 95), 200)\n",
        "accuracies = [accuracy_score(y_test_sub, (scores > t).astype(int)) for t in thresholds]\n",
        "best_idx = np.argmax(accuracies)\n",
        "\n",
        "best_thresh = thresholds[best_idx]\n",
        "final_preds = (scores > best_thresh).astype(int)\n",
        "\n",
        "print(\"=\"*40)\n",
        "print(f\"[Minimalist HMM] Best Threshold: {best_thresh:.2f} | Acc: {accuracies[best_idx]:.4f}\")\n",
        "print(\"-\" * 40)\n",
        "print(classification_report(y_test_sub, final_preds, target_names=['Neg', 'Pos']))"
      ],
      "metadata": {
        "id": "VEN5_nJfUlHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# implementation with LLM\n"
      ],
      "metadata": {
        "id": "euw05mTFOhYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "MAX_LEN = 128\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "print(\"Tokenizando dataset...\")\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tf_train = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
        "    label_cols=[\"label\"],\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "tf_test = tokenized_datasets[\"test\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
        "    label_cols=[\"label\"],\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "id": "J2NvEAHL48ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=2,\n",
        "    force_download=True,\n",
        "    use_safetensors=False\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "fZKlX8QJ4_pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    tf_train,\n",
        "    validation_data=tf_test,\n",
        "    epochs=4\n",
        ")"
      ],
      "metadata": {
        "id": "0BKnMzXD5C_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Treino Acc')\n",
        "    plt.plot(epochs_range, val_acc, label='Validação Acc')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Acurácia de Treino e Validação')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Treino Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validação Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Perda de Treino e Validação')\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "L5q8YqQa5Gbi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}